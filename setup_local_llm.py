#!/usr/bin/env python3
"""
Setup script for local LLM integration
Helps users configure Ollama or Google Gemini for the trading experiment
"""

import os
import sys
import subprocess
import requests
from pathlib import Path

def check_python_packages():
    """Check if required Python packages are installed"""
    required_packages = [
        'pandas', 'yfinance', 'matplotlib', 'numpy', 'requests'
    ]
    
    optional_packages = [
        ('google-generativeai', 'Google Gemini support'),
        ('python-dotenv', 'Environment variable management'),
        ('ollama', 'Ollama Python client (optional)')
    ]
    
    print("üîç Checking Python packages...")
    
    missing_required = []
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"‚úÖ {package}")
        except ImportError:
            print(f"‚ùå {package}")
            missing_required.append(package)
    
    if missing_required:
        print(f"\n‚ö†Ô∏è  Missing required packages: {', '.join(missing_required)}")
        print("Install with: pip install " + " ".join(missing_required))
        return False
    
    print("\nüîç Checking optional packages...")
    for package, description in optional_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"‚úÖ {package} - {description}")
        except ImportError:
            print(f"‚ö™ {package} - {description} (optional)")
    
    return True

def check_ollama_installation():
    """Check if Ollama is installed and running"""
    print("\nüîç Checking Ollama installation...")
    
    # Check if Ollama is installed
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print(f"‚úÖ Ollama installed: {result.stdout.strip()}")
        else:
            print("‚ùå Ollama not found in PATH")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print("‚ùå Ollama not installed or not in PATH")
        print("üì• Install Ollama from: https://ollama.ai/")
        return False
    
    # Check if Ollama is running
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama service is running")
            
            # List available models
            models = response.json().get('models', [])
            if models:
                print("üìö Available models:")
                for model in models:
                    print(f"   - {model['name']}")
            else:
                print("‚ö†Ô∏è  No models installed")
                print("üí° Install a model with: ollama pull llama2")
            return True
        else:
            print("‚ùå Ollama service not responding")
            return False
    except requests.exceptions.RequestException:
        print("‚ùå Cannot connect to Ollama service")
        print("üí° Start Ollama with: ollama serve")
        return False

def setup_environment_file():
    """Create .env file if it doesn't exist"""
    env_file = Path('.env')
    env_example = Path('.env.example')
    
    if env_file.exists():
        print("‚úÖ .env file already exists")
        return True
    
    if env_example.exists():
        print("üìù Creating .env file from template...")
        with open(env_example, 'r') as f:
            content = f.read()
        
        with open(env_file, 'w') as f:
            f.write(content)
        
        print("‚úÖ .env file created")
        print("‚ö†Ô∏è  Please edit .env file and add your API keys")
        return True
    else:
        print("‚ö†Ô∏è  .env.example not found, creating basic .env file...")
        basic_env = """# Google Gemini API Configuration
GOOGLE_API_KEY=your_google_api_key_here

# Ollama Configuration (for local models)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2

# Trading configuration
PORTFOLIO_UPDATE_INTERVAL=daily
RISK_TOLERANCE=moderate
"""
        with open(env_file, 'w') as f:
            f.write(basic_env)
        
        print("‚úÖ Basic .env file created")
        print("‚ö†Ô∏è  Please edit .env file and configure your settings")
        return True

def test_llm_integration():
    """Test LLM integration"""
    print("\nüß™ Testing LLM integration...")
    
    try:
        from Scripts.llm_integration import LLMPortfolioAnalyzer
        
        # Test Ollama
        try:
            analyzer = LLMPortfolioAnalyzer(provider="ollama")
            response = analyzer.generate_response("Hello, this is a test. Please respond with 'Ollama is working!'")
            if "working" in response.lower():
                print("‚úÖ Ollama integration working")
            else:
                print(f"‚ö†Ô∏è  Ollama responded but unexpected output: {response[:100]}...")
        except Exception as e:
            print(f"‚ùå Ollama test failed: {e}")
        
        # Test Gemini (if API key is set)
        try:
            if os.getenv('GOOGLE_API_KEY') and os.getenv('GOOGLE_API_KEY') != 'your_google_api_key_here':
                analyzer = LLMPortfolioAnalyzer(provider="gemini")
                response = analyzer.generate_response("Hello, this is a test. Please respond with 'Gemini is working!'")
                if "working" in response.lower():
                    print("‚úÖ Gemini integration working")
                else:
                    print(f"‚ö†Ô∏è  Gemini responded but unexpected output: {response[:100]}...")
            else:
                print("‚ö™ Gemini test skipped (no API key configured)")
        except Exception as e:
            print(f"‚ùå Gemini test failed: {e}")
            
    except ImportError as e:
        print(f"‚ùå Cannot import LLM integration module: {e}")
        return False
    
    return True

def main():
    """Main setup function"""
    print("üöÄ ChatGPT Micro-Cap Experiment - Local LLM Setup")
    print("="*60)
    
    # Check Python packages
    if not check_python_packages():
        print("\n‚ùå Setup failed: Missing required packages")
        sys.exit(1)
    
    # Check Ollama
    ollama_ok = check_ollama_installation()
    
    # Setup environment
    setup_environment_file()
    
    # Test integration
    test_llm_integration()
    
    print("\n" + "="*60)
    print("üìã SETUP SUMMARY")
    print("="*60)
    
    if ollama_ok:
        print("‚úÖ Ollama setup complete - You can use local models")
    else:
        print("‚ö†Ô∏è  Ollama not ready - Install and configure Ollama for local models")
    
    print("‚úÖ Environment file created - Configure API keys as needed")
    print("‚úÖ LLM integration modules ready")
    
    print("\nüìö NEXT STEPS:")
    print("1. If using Ollama: Install a model with 'ollama pull llama2'")
    print("2. If using Gemini: Add your GOOGLE_API_KEY to .env file")
    print("3. Run the enhanced trading script: python Scripts/enhanced_trading_script.py")
    print("4. Or test LLM integration: python Scripts/llm_integration.py")
    
    print("\nüéØ The repository now supports local LLMs and Google Gemini!")
    print("   No OpenAI dependency required.")

if __name__ == "__main__":
    main()